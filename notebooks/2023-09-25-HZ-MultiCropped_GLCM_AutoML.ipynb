{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023 06/16 Multi Cropped Palm and LBP AutoML Experimentation\n",
    "\n",
    "*Last Updated*: 2023-09-26\n",
    "\n",
    "### Authors\n",
    "* Hannah Zhang (hannahzhang@ucsb.edu)\n",
    "\n",
    "\n",
    "### Overview\n",
    "This Jupyter notebook is intended to demonstrate\n",
    "\n",
    "* Mediapipe multi palm cropped images with lbp feature extraction results from automl tests\n",
    "\n",
    "\n",
    "### Key Results\n",
    "\n",
    "- Linear Regressor, Bayesian Ridge, and Light Gradient Boosting Machine performed the best\n",
    "- Dummy Regressor did not perform well as compared to experiments with lbp features from single cropped palm -> more lbp features per hand leads to better results with automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports\n",
    "\n",
    "# External packages\n",
    "from pycaret import regression\n",
    "from pycaret.datasets import get_data\n",
    "import os\n",
    "import cv2\n",
    "from dermaml import data, features\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"11khands\"\n",
    "\n",
    "experiment_name = \"multi-cropped 11khands with lbp\"\n",
    "\n",
    "num_best_models = 5\n",
    "random_seed = 345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset_name = \"11khands\"\n",
    "\n",
    "# AutoML\n",
    "experiment_name = \"11khands-automl-sample-test\"\n",
    "num_best_models = 5\n",
    "random_seed = 123  # seed used for random number generators to ensure reproducibility of results in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def load_image_paths(folder_path):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff')):\n",
    "                # Create the full path to the image file\n",
    "                image_path = os.path.join(root, file)\n",
    "                image_paths.append(image_path)\n",
    "    return image_paths\n",
    "\n",
    "# Example usage:\n",
    "image_folder = '/Users/hannahzhang/Downloads/11khands_test_ims_2/'\n",
    "image_paths = load_image_paths(image_folder)\n",
    "\n",
    "# Print the list of image paths\n",
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for path in image_paths: \n",
    "    result = data.multi_crop_palm(path)\n",
    "    if result is not None:\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the list of dictionaries\n",
    "for result in results:\n",
    "    # Iterate through the dictionary to access image filenames and sub-dictionaries\n",
    "    for image_filename, image_dict in result.items():\n",
    "        # Iterate through the sub-dictionary containing the images\n",
    "        for key, image in image_dict.items():\n",
    "            # Check if the key is 'Image 9' or 'Image 10' to process images\n",
    "            if key == 'Image 9' or key == 'Image 10':\n",
    "                # Convert the image to grayscale\n",
    "                grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Update the sub-dictionary with the grayscale image\n",
    "                image_dict[key] = grayscale_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store LBP features\n",
    "lbp_features = []\n",
    "\n",
    "# Iterate through the list of dictionaries containing images\n",
    "for image_dict in results:\n",
    "    # Extract the image filenames\n",
    "    image_filenames = list(image_dict.keys())\n",
    "    \n",
    "    # Iterate through the sub-dictionaries containing the images\n",
    "    for image_filename, image_data in image_dict.items():\n",
    "        # Extract the images from the sub-dictionary\n",
    "        image_9 = image_data['Image 9']\n",
    "        image_10 = image_data['Image 10']\n",
    "\n",
    "        # Apply LBP feature extraction to images\n",
    "        lbp_9 = features.extract_features(image_9)\n",
    "        lbp_10 = features.extract_features(image_10)\n",
    "\n",
    "        # Append the LBP features and filenames to the lbp_features list\n",
    "        lbp_features.append({\n",
    "            'landmark_9': lbp_9,\n",
    "            'landmark_10': lbp_10,\n",
    "            'filename': image_filename\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lbp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(lbp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         lbp_df\u001b[38;5;241m.\u001b[39mat[i, column_name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Add the filename column\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m lbp_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlbp_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Reorder the columns\u001b[39;00m\n\u001b[1;32m     24\u001b[0m column_order \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpattern \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lbp_df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame\n",
    "lbp_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through the list of dictionaries and populate the DataFrame\n",
    "for i, item in enumerate(lbp_features):\n",
    "    filename = item['filename']\n",
    "    landmark_9_values = item['landmark_9']['texture']\n",
    "    landmark_10_values = item['landmark_10']['texture']\n",
    "\n",
    "    # Create columns for landmark 9 values\n",
    "    for j, value in enumerate(landmark_9_values):\n",
    "        column_name = f'pattern {j + 1}'\n",
    "        lbp_df.at[i, column_name] = value\n",
    "\n",
    "    # Create columns for landmark 10 values\n",
    "    for j, value in enumerate(landmark_10_values):\n",
    "        column_name = f'pattern {j + 1 + len(landmark_9_values)}'\n",
    "        lbp_df.at[i, column_name] = value\n",
    "\n",
    "# Add the filename column\n",
    "lbp_df['filename'] = lbp_features[0]['filename']\n",
    "\n",
    "# Reorder the columns\n",
    "column_order = ['filename'] + [f'pattern {i + 1}' for i in range(len(lbp_df.columns) - 1)]\n",
    "lbp_df = lbp_df[column_order]\n",
    "\n",
    "print(lbp_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '/Users/hannahzhang/Downloads/multi_cropped_lbp_automl.csv'\n",
    "\n",
    "original_df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(original_df, lbp_df, left_on='imageName', right_on='filename', how='inner')\n",
    "\n",
    "# Drop filename column\n",
    "\n",
    "merged_df.drop('filename', axis=1, inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop('imageName', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Columns: {list(merged_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Perform AutoML Evaluation\n",
    "\n",
    "# Set up the dataset for AutoML regression\n",
    "regression.setup(data=merged_df,\n",
    "                 target=\"age\",\n",
    "                 log_experiment=True,\n",
    "                 experiment_name=experiment_name,\n",
    "                 session_id=random_seed,\n",
    "                ) \n",
    "\n",
    "# Automatically train, test, and evaluate models\n",
    "best_models = regression.compare_models(n_select=num_best_models, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in best_models:\n",
    "    print(model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display score table\n",
    "regression.pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Linear Regression\n",
    "\n",
    "lr_model = regression.create_model('lr')\n",
    "lr_model_tuned = regression.tune_model(lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bayesian Ridge\n",
    "\n",
    "br_model = regression.create_model('br')\n",
    "br_model_tuned = regression.tune_model(br_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
